################################################################################
# Environment Configuration File for LuminariMUD
################################################################################
#
# This is an example configuration file for LuminariMUD environment variables.
#
# CRITICAL: FILE LOCATION REQUIREMENT
# ===================================
# The '.env' file MUST be placed in the lib/ directory!
# The MUD looks for this file at: lib/.env
#
# To use this file:
#   1. Copy this file to 'lib/.env' (without the .example suffix)
#      Example: cp .env.example lib/.env
#   2. Edit the values below with your actual configuration
#   3. Ensure the .env file has appropriate permissions (chmod 600)
#      Example: chmod 600 lib/.env
#
# IMPORTANT: The actual '.env' file is git-ignored to protect your
#            API keys and credentials. Never commit real credentials to version control!
#
################################################################################

# Application Environment
# Options: development, testing, production
APP_ENV=production

# Application Timezone
APP_TIMEZONE=UTC

# Database Configuration
DB_HOST=localhost
DB_PORT=3306
DB_NAME=luminarimud
DB_USER=your_username
DB_PASS=your_password

# Security Configuration
# Generate a random 32-character string for this
APP_SECRET=your_32_character_secret_key_here

# Session Configuration
SESSION_LIFETIME=3600

# Cache Configuration
CACHE_ENABLED=true
CACHE_TTL=3600

# Logging Configuration
LOG_LEVEL=error
LOG_FILE=logs/application.log

# Authentication Configuration
# Set to true to require authentication for all tools
REQUIRE_AUTH=false

# Admin Configuration
# Default admin credentials (change these!)
ADMIN_USERNAME=admin
ADMIN_PASSWORD=change_this_password

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=3600

# File Upload Configuration
MAX_UPLOAD_SIZE=10M
ALLOWED_EXTENSIONS=txt,csv,json

# Email Configuration (for notifications)
MAIL_HOST=localhost
MAIL_PORT=587
MAIL_USERNAME=
MAIL_PASSWORD=
MAIL_FROM=noreply@yourdomain.com

# Backup Configuration
BACKUP_ENABLED=false
BACKUP_PATH=backups/
BACKUP_RETENTION_DAYS=30

################################################################################
# AI Service Configuration
################################################################################
#
# The AI service provides intelligent NPC dialogue using either OpenAI (cloud)
# or Ollama (local). Ollama is free and runs locally, OpenAI requires an API key.
#
# QUICK START:
#   - For FREE local AI: Install Ollama, run 'ollama pull llama3.2:1b'
#   - For cloud AI: Set OPENAI_API_KEY below
#   - In-game: Use 'ai enable' to activate, 'ai' to check status
#

#-------------------------------------------------------------------------------
# General AI Settings
#-------------------------------------------------------------------------------

# Enable debug logging for AI service (0=off, 1=on)
# - Produces verbose logs useful for troubleshooting
# - WARNING: Creates significant log output, disable in production
AI_DEBUG_MODE=0

# Maximum retry attempts when API calls fail
# - Higher values improve reliability but increase latency on failures
# - Default: 3 retries with exponential backoff
AI_MAX_RETRIES=3

# Maximum cached AI responses
# - Higher values use more memory but improve hit rate
# - Each cached response is ~500-2000 bytes
# - Default: 5000 entries (~5-10MB memory)
AI_MAX_CACHE_SIZE=5000

# Cache expiration time (in seconds)
# - How long to cache AI responses before they expire
# - Higher values reduce API calls but responses may become stale
# - 3600 = 1 hour (default)
AI_CACHE_EXPIRE_SECONDS=3600

# Content Filtering
# - Enable to filter inappropriate content from AI responses
# - Recommended for public servers
AI_CONTENT_FILTER_ENABLED=true

#-------------------------------------------------------------------------------
# OpenAI Configuration (Cloud AI - Requires API Key)
#-------------------------------------------------------------------------------

# OpenAI API key for AI-powered NPC dialogue
# - Get your API key from: https://platform.openai.com/api-keys
# - Format: sk-proj-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
# - Keep this secret! Anyone with this key can use your OpenAI account
# - Leave empty to use Ollama-only mode (free, local)
OPENAI_API_KEY=

# OpenAI API Endpoint
# - Default: https://api.openai.com/v1/chat/completions
# - Change only if using a custom/proxy endpoint
OPENAI_API_ENDPOINT=https://api.openai.com/v1/chat/completions

# AI Model Selection for OpenAI
# - Available models: gpt-4.1-mini, gpt-4o-mini, gpt-4.1, gpt-4o
# - gpt-4.1-mini is fastest and most cost-effective (83% cheaper than gpt-4o)
# - gpt-4o-mini is also very cost-effective ($0.15/1M input, $0.60/1M output tokens)
# - gpt-4.1 provides best quality with 1M token context window
AI_MODEL=gpt-5-mini

# Maximum Response Length for OpenAI (in tokens)
# - 1 token = approximately 4 characters in English
# - Higher values = longer responses but higher costs
# - Recommended: 100-500 for NPC dialogue
AI_MAX_TOKENS=500

# Response Creativity for OpenAI (0.0-1.0, stored as integer 0-10)
# - 0 = Very focused and deterministic (good for factual responses)
# - 3 = Low creativity (default, good for consistent NPC behavior)
# - 7 = Balanced creativity
# - 10 = Very creative but may be unpredictable
# NOTE: Value is divided by 10 (e.g., 7 = 0.7 temperature)
AI_TEMPERATURE=3

# OpenAI Request Timeout (in milliseconds)
# - How long to wait for OpenAI API response
# - Higher values handle slow connections but may delay gameplay
# - Range: 1 to 300000 (5 minutes max)
# - Default: 30000 (30 seconds)
AI_TIMEOUT_MS=30000

#-------------------------------------------------------------------------------
# Rate Limiting (Applies to OpenAI)
#-------------------------------------------------------------------------------

# Requests per minute limit
# - Prevents excessive API usage and cost overruns
# - Adjust based on your OpenAI plan limits
# - Default: 60 requests/minute
AI_REQUESTS_PER_MINUTE=60

# Requests per hour limit
# - Secondary limit to control hourly costs
# - Default: 1000 requests/hour
AI_REQUESTS_PER_HOUR=1000

#-------------------------------------------------------------------------------
# Ollama Configuration (Local AI - Free, No API Key Required)
#-------------------------------------------------------------------------------

# Ollama API Endpoint
# - Default: http://localhost:11434/api/generate
# - Change if Ollama is running on a different host/port
OLLAMA_API_ENDPOINT=http://localhost:11434/api/generate

# Ollama Model
# - Model to use for local AI generation
# - Recommended models by size/speed:
#   - llama3.2:1b  (1.3GB, fastest, good for NPCs)
#   - tinyllama    (637MB, very fast, basic responses)
#   - llama3.2:3b  (2GB, better quality, moderate speed)
#   - mistral:7b   (4GB, high quality, slower)
# - Install with: ollama pull <model_name>
OLLAMA_MODEL=llama3.2:1b

# Ollama Request Timeout (in milliseconds)
# - How long to wait for Ollama response
# - Local models may need extra time on first request (model loading)
# - Default: 10000 (10 seconds)
OLLAMA_TIMEOUT_MS=10000

# Ollama Maximum Tokens (num_predict)
# - Maximum tokens per Ollama response
# - Lower values = faster responses
# - Default: 100 tokens for quick NPC dialogue
OLLAMA_MAX_TOKENS=100

# Ollama Temperature (0.0-1.0, stored as integer 0-10)
# - Controls response creativity for Ollama
# - 7 = 0.7 temperature (default, balanced)
# NOTE: Value is divided by 10
OLLAMA_TEMPERATURE=7

# Ollama Top-K Sampling
# - Limits vocabulary to top K most likely tokens
# - Lower values = more focused responses
# - Default: 40
OLLAMA_TOP_K=40

# Ollama Top-P (Nucleus) Sampling (0.0-1.0, stored as integer 0-100)
# - Cumulative probability threshold for token selection
# - 90 = 0.9 (default, includes top 90% probability mass)
# NOTE: Value is divided by 100
OLLAMA_TOP_P=90

#######################################################################
# I3 Gateway Connectoin
#######################################################################

#I3_GATEWAY_URL=wss://domain.of.i3.gateway
#I3_API_KEY=api-key-for-this-mud
#I3_MUD_NAME=LuminariMUD
# this has been moved to i3_config file
