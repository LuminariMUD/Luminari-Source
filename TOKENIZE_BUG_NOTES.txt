TOKENIZE BUG ANALYSIS - Comparison of Old vs New Versions
=========================================================

ORIGINAL TOKENIZE FUNCTION (from LuminariMUD_AI_FRIENDLY_PACK.xml line 249733):
-------------------------------------------------------------------------------
/* String tokenizer. */
char **tokenize(const char *input, const char *delim)
{
  char *str = strdup(input);
  int count = 0;
  int capacity = 10;
  char **result = malloc(capacity * sizeof(*result));

  char *tok = strtok(str, delim);

  while (1)    // <--- INFINITE LOOP!
  {
    if (count >= capacity)
      result = realloc(result, (capacity *= 2) * sizeof(*result));

    result[count++] = tok ? strdup(tok) : tok;   // <--- STORES NULL!

    if (!tok)    // <--- BREAKS AFTER STORING NULL
      break;

    tok = strtok(NULL, delim);
  }

  free(str);
  return result;
}

CURRENT TOKENIZE FUNCTION (with error handling added):
------------------------------------------------------
/* String tokenizer. */
char **tokenize(const char *input, const char *delim)
{
  char *str;
  char **result;
  char *tok;
  int count = 0;
  int capacity = 10;
  
  /* Sanity check inputs */
  if (!input || !delim) {
    log("SYSERR: tokenize() called with NULL input or delim");
    return NULL;
  }
  
  str = strdup(input);
  if (!str) {
    log("SYSERR: tokenize() failed to allocate memory for input string");
    return NULL;
  }
  
  /* Allocate space including room for NULL terminator */
  result = malloc((capacity + 1) * sizeof(*result));
  if (!result) {
    log("SYSERR: tokenize() failed to allocate memory for result array");
    free(str);
    return NULL;
  }

  tok = strtok(str, delim);

  while (tok)    // <--- CORRECT: Only loops while tok is not NULL
  {
    char *dup;
    
    if (count >= capacity) {
      /* ... realloc code with error handling ... */
    }

    dup = strdup(tok);
    if (!dup) {
      /* ... error handling ... */
    }
    result[count++] = dup;
    tok = strtok(NULL, delim);
  }
  
  /* NULL-terminate the array */
  result[count] = NULL;

  free(str);
  return result;
}

KEY DIFFERENCES AND BUGS IN ORIGINAL:
=====================================

1. INFINITE LOOP BUG (CRITICAL):
   - Original uses `while (1)` - infinite loop!
   - Stores token BEFORE checking if it's NULL
   - This means it ALWAYS stores NULL as the last element BEFORE the break

2. NULL TERMINATION BUG:
   - Original increments count AFTER storing NULL: `result[count++] = tok`
   - This means the NULL is stored at result[count-1], not at result[count]
   - The array is NOT properly NULL-terminated!
   - This causes free_tokens() to read past the NULL into uninitialized memory

3. NO ERROR HANDLING:
   - No checks for malloc/realloc/strdup failures
   - No input validation
   - No cleanup on errors

4. MEMORY ALLOCATION:
   - Original allocates exactly 'capacity' elements
   - New version allocates 'capacity + 1' to ensure room for NULL terminator

THE ACTUAL BUG CAUSING EMPTY FIRST TOKEN:
=========================================

Looking at the original code, I don't see how it would produce an empty first token 
UNLESS the input actually starts with the delimiter.

However, the NULL termination bug in the original could cause all sorts of 
undefined behavior when free_tokens() tries to iterate past the improperly 
terminated array.

POSSIBLE CAUSES FOR EMPTY FIRST TOKEN:
--------------------------------------
1. Input data from MySQL actually starts with '\n' (newline)
2. Character encoding issue (BOM, carriage return, etc.)
3. MySQL is adding a newline when storing/retrieving TEXT fields
4. The serialization format has changed and now includes leading newline

RECOMMENDED DEBUGGING:
---------------------
1. Check the raw data in MySQL:
   SELECT HEX(SUBSTRING(serialized_obj, 1, 10)) FROM house_data WHERE vnum = 24828;
   
2. Add logging to see raw input in tokenize():
   log("First 10 chars hex: %02X %02X %02X ...", input[0], input[1], ...);

3. Temporarily trim leading whitespace before tokenizing:
   while (*input && (*input == '\n' || *input == '\r' || *input == ' '))
     input++;

CONCLUSION:
-----------
The new tokenize() function is definitely better with proper error handling and 
correct loop termination. However, the empty first token issue is likely caused by:
- The input data having a leading newline that shouldn't be there
- OR a change in how MySQL stores/retrieves TEXT data

The original tokenize() had serious bugs but wouldn't cause empty first token 
unless the input starts with delimiter.

DEBUG OUTPUT ANALYSIS (Jul 25 09:19)
====================================

GOOD NEWS - tokenize() is working correctly!
--------------------------------------------
The debug output shows tokenize is being called with various inputs and working properly:

Examples from regions/paths loading:
- Input: '-57 9' → First token: '-57 93' ✓
- Input: '1 0,1' → First token: '1 0' ✓  
- Input: '623.7' → First token: '623.75 114.75' ✓
- Input: '3 35,' → First token: '3 35' ✓

All inputs start with valid characters (no leading newlines).
All tokens are extracted correctly.

THE REAL BUG - HARDCODED TEST IS CAUSING CORRUPTION!
----------------------------------------------------
When loading houses, the hardcoded test is ENABLED:
```
Jul 25 09:19:40 :: DEBUG: Using hardcoded tokenization for testing
Jul 25 09:19:40 :: Unknown tag in saved obj: Ou^T
Jul 25 09:19:40 :: DEBUG: Using hardcoded tokenization for testing  
Jul 25 09:19:40 :: Unknown tag in saved obj: Qu^T
```

The hardcoded test in objsave.c is returning the SAME array for EVERY object:
```c
if (0) {  /* Set to 1 to enable hardcoded test */
  lines = malloc(5 * sizeof(char*));
  lines[0] = strdup("#3183");
  lines[1] = strdup("Loc : -1");
  lines[2] = strdup("Flag: 64 0 0 0");
  lines[3] = strdup("Name: a small leather pouch");
  lines[4] = NULL;
}
```

But the code is set to `if (0)` which means it SHOULDN'T run!

CRITICAL FINDING:
-----------------
The debug message "Using hardcoded tokenization for testing" is being printed, 
but the actual hardcoded array is NOT being used properly. This suggests:

1. The `if (0)` was changed to `if (1)` to enable the test
2. But the parsing code is getting corrupted data anyway
3. The "Unknown tag" messages show garbage: `Ou^T`, `Qu^T`, etc.

The pattern `u^T` suggests memory corruption or uninitialized memory being read.

ROOT CAUSE:
-----------
The hardcoded test is incomplete! It only provides 4 lines for a pouch, but 
the parser expects MANY more lines for a complete object (values, affects, etc).
When the parser reads past line 4, it's reading uninitialized memory!

SOLUTION:
---------
1. DISABLE the hardcoded test: Change `if (1)` back to `if (0)` in objsave.c
2. The original tokenize() is actually working fine for regions/paths
3. The crash is from the incomplete hardcoded test data, not tokenize()!

The garbage tags (Ou^T, Qu^T, etc.) are ASCII values incrementing (O=79, Q=81, etc.)
which strongly suggests reading sequential uninitialized memory.

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!! CRITICAL UPDATE: THE PROBLEM HAS NOT BEEN RESOLVED                     !!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

THE SERVER IS STILL CRASHING WITH THE HARDCODED TEST DISABLED.

The debug output showed:
1. tokenize() works correctly for regions/paths (no empty tokens)
2. The hardcoded test was causing corruption when enabled
3. But with hardcoded test DISABLED, the server STILL CRASHES

THIS MEANS THE ORIGINAL CRASH IS STILL HAPPENING.

The tokenize() function has been improved with error handling, but this did NOT
fix the underlying issue causing the server to crash when loading house data.

NEXT STEPS NEEDED:
1. Need to see ACTUAL tokenize() output when loading house data (not regions)
2. Need to check the raw MySQL data for house #24828
3. May need to add more targeted debugging specifically for house loading
4. The empty first token issue reported by GDB is STILL UNEXPLAINED

THE BUG REMAINS UNRESOLVED.

---

## COMPREHENSIVE DEBUGGING TASK LIST

### Priority 1: Immediate Diagnostics

#### 1. **Isolate House #24828**
- Delete house #24828 from the database temporarily
- `DELETE FROM house_data WHERE vnum = 24828;`
- If server boots successfully, the issue is specific to this house's data

#### 2. **Binary Search Debug**
- Comment out half of the object parsing code in objsave.c
- If it crashes, comment out half of the remaining code
- Continue until the exact crashing line is identified

#### 3. **Validate Database Data**
- Check for hidden Unicode characters:
  `SELECT LENGTH(serialized_obj), CHAR_LENGTH(serialized_obj) FROM house_data WHERE vnum = 24828;`
- If values differ, Unicode characters may be present
- Check data size: `SELECT LENGTH(serialized_obj) FROM house_data WHERE vnum = 24828;`

#### 4. **Memory Corruption Analysis**
- Run under Valgrind: `valgrind --leak-check=full --track-origins=yes bin/circle -q 4101`
- Check for memory corruption occurring before tokenize()

### Priority 2: String Handling Issues

#### 5. **Test Minimal tokenize() Implementation**
```c
// Replace tokenize function temporarily with:
char **tokenize(const char *input, const char *delim) {
    char **result = malloc(2 * sizeof(char*));
    result[0] = strdup("TEST_TOKEN");
    result[1] = NULL;
    return result;
}
```

#### 6. **Thread Safety Check**
- Replace strtok() with strtok_r() for thread-safe tokenization
- strtok() maintains internal state that could be corrupted

#### 7. **Alternative String Duplication**
- Replace strdup() calls with malloc+strcpy
- Some systems have non-standard strdup implementations

### Priority 3: Environmental Factors

#### 8. **Compiler Optimization Issues**
- Recompile with no optimization: `make clean && make CFLAGS="-O0 -g"`
- Modern compilers may optimize assuming no undefined behavior

#### 9. **Stack Size Limitations**
- Increase stack size: `ulimit -s unlimited`
- Check for stack overflow in other parts of the code

#### 10. **Clean Rebuild**
- Delete all object files: `make clean`
- Rebuild entire project: `make`
- Ensures no stale object files

### Priority 4: Database Investigation

#### 11. **Compare Working vs Failing Houses**
- Find a house that loads successfully
- Compare its serialized_obj structure with house #24828
- Identify structural differences

#### 12. **Character Set Analysis**
- Check database character settings: `SHOW VARIABLES LIKE 'character_set%';`
- Ensure consistent encoding between database and application

#### 13. **MySQL Connection Validation**
- Verify connection is active when loading houses
- Add mysql_ping() before queries to ensure connection health

#### 14. **TEXT Field Limitations**
- TEXT fields have 65,535 byte limit
- Check if data is being truncated

### Priority 5: Alternative Approaches

#### 15. **Bypass House Loading Temporarily**
- Comment out house loading in House_load()
- Return early to get server running
- Debug house loading separately

#### 16. **Test Adjacent Houses**
- Try loading house #24827 or #24829
- Determine if issue is specific to one house

#### 17. **Alternative Memory Allocators**
- Test with jemalloc: `LD_PRELOAD=/usr/lib/libjemalloc.so bin/circle -q 4101`
- Different allocators may handle edge cases differently

### Priority 6: Deep Debugging

#### 18. **Macro Redefinition Check**
- Search all headers for redefinitions of free(), malloc(), strdup()
- `grep -r "#define free" *.h`

#### 19. **Raw Data Inspection**
- Extract raw hex data: `SELECT HEX(SUBSTRING(serialized_obj, 1, 100)) FROM house_data WHERE vnum = 24828;`
- Look for unexpected bytes at start of data

#### 20. **Progressive Return Strategy**
- Add early returns throughout the code path
- Start with `return NULL;` at tokenize() entry
- Move return statement down until crash occurs
- Identifies exact location of problem

### Priority 7: External Testing

#### 21. **Different Environment Test**
- Test on different Linux distribution/version
- Environment-specific bugs are possible

#### 22. **MySQL Library Version**
- Check MySQL client library version
- Try static linking with older version if available

### Key Observations
- The bug survives multiple fixes, suggesting the root cause is not where expected
- Empty first token issue remains unexplained
- Hardcoded test corruption was a red herring
- Server still crashes with improved error handling

### Next Immediate Steps
1. Run Priority 1 tasks to isolate the problem
2. If house #24828 specific, examine its data closely
3. If not house specific, focus on memory corruption analysis
4. Document findings at each step for pattern recognition