TOKENIZE BUG ANALYSIS - Comparison of Old vs New Versions
=========================================================

ORIGINAL TOKENIZE FUNCTION (from LuminariMUD_AI_FRIENDLY_PACK.xml line 249733):
-------------------------------------------------------------------------------
/* String tokenizer. */
char **tokenize(const char *input, const char *delim)
{
  char *str = strdup(input);
  int count = 0;
  int capacity = 10;
  char **result = malloc(capacity * sizeof(*result));

  char *tok = strtok(str, delim);

  while (1)    // <--- INFINITE LOOP!
  {
    if (count >= capacity)
      result = realloc(result, (capacity *= 2) * sizeof(*result));

    result[count++] = tok ? strdup(tok) : tok;   // <--- STORES NULL!

    if (!tok)    // <--- BREAKS AFTER STORING NULL
      break;

    tok = strtok(NULL, delim);
  }

  free(str);
  return result;
}

CURRENT TOKENIZE FUNCTION (with error handling added):
------------------------------------------------------
/* String tokenizer. */
char **tokenize(const char *input, const char *delim)
{
  char *str;
  char **result;
  char *tok;
  int count = 0;
  int capacity = 10;
  
  /* Sanity check inputs */
  if (!input || !delim) {
    log("SYSERR: tokenize() called with NULL input or delim");
    return NULL;
  }
  
  str = strdup(input);
  if (!str) {
    log("SYSERR: tokenize() failed to allocate memory for input string");
    return NULL;
  }
  
  /* Allocate space including room for NULL terminator */
  result = malloc((capacity + 1) * sizeof(*result));
  if (!result) {
    log("SYSERR: tokenize() failed to allocate memory for result array");
    free(str);
    return NULL;
  }

  tok = strtok(str, delim);

  while (tok)    // <--- CORRECT: Only loops while tok is not NULL
  {
    char *dup;
    
    if (count >= capacity) {
      /* ... realloc code with error handling ... */
    }

    dup = strdup(tok);
    if (!dup) {
      /* ... error handling ... */
    }
    result[count++] = dup;
    tok = strtok(NULL, delim);
  }
  
  /* NULL-terminate the array */
  result[count] = NULL;

  free(str);
  return result;
}

KEY DIFFERENCES AND BUGS IN ORIGINAL:
=====================================

1. INFINITE LOOP BUG (CRITICAL):
   - Original uses `while (1)` - infinite loop!
   - Stores token BEFORE checking if it's NULL
   - This means it ALWAYS stores NULL as the last element BEFORE the break

2. NULL TERMINATION BUG:
   - Original increments count AFTER storing NULL: `result[count++] = tok`
   - This means the NULL is stored at result[count-1], not at result[count]
   - The array is NOT properly NULL-terminated!
   - This causes free_tokens() to read past the NULL into uninitialized memory

3. NO ERROR HANDLING:
   - No checks for malloc/realloc/strdup failures
   - No input validation
   - No cleanup on errors

4. MEMORY ALLOCATION:
   - Original allocates exactly 'capacity' elements
   - New version allocates 'capacity + 1' to ensure room for NULL terminator

THE ACTUAL BUG CAUSING EMPTY FIRST TOKEN:
=========================================

Looking at the original code, I don't see how it would produce an empty first token 
UNLESS the input actually starts with the delimiter.

However, the NULL termination bug in the original could cause all sorts of 
undefined behavior when free_tokens() tries to iterate past the improperly 
terminated array.

POSSIBLE CAUSES FOR EMPTY FIRST TOKEN:
--------------------------------------
1. Input data from MySQL actually starts with '\n' (newline)
2. Character encoding issue (BOM, carriage return, etc.)
3. MySQL is adding a newline when storing/retrieving TEXT fields
4. The serialization format has changed and now includes leading newline

RECOMMENDED DEBUGGING:
---------------------
1. Check the raw data in MySQL:
   SELECT HEX(SUBSTRING(serialized_obj, 1, 10)) FROM house_data WHERE vnum = 24828;
   
2. Add logging to see raw input in tokenize():
   log("First 10 chars hex: %02X %02X %02X ...", input[0], input[1], ...);

3. Temporarily trim leading whitespace before tokenizing:
   while (*input && (*input == '\n' || *input == '\r' || *input == ' '))
     input++;

CONCLUSION:
-----------
The new tokenize() function is definitely better with proper error handling and 
correct loop termination. However, the empty first token issue is likely caused by:
- The input data having a leading newline that shouldn't be there
- OR a change in how MySQL stores/retrieves TEXT data

The original tokenize() had serious bugs but wouldn't cause empty first token 
unless the input starts with delimiter.

DEBUG OUTPUT ANALYSIS (Jul 25 09:19)
====================================

GOOD NEWS - tokenize() is working correctly!
--------------------------------------------
The debug output shows tokenize is being called with various inputs and working properly:

Examples from regions/paths loading:
- Input: '-57 9' → First token: '-57 93' ✓
- Input: '1 0,1' → First token: '1 0' ✓  
- Input: '623.7' → First token: '623.75 114.75' ✓
- Input: '3 35,' → First token: '3 35' ✓

All inputs start with valid characters (no leading newlines).
All tokens are extracted correctly.

THE REAL BUG - HARDCODED TEST IS CAUSING CORRUPTION!
----------------------------------------------------
When loading houses, the hardcoded test is ENABLED:
```
Jul 25 09:19:40 :: DEBUG: Using hardcoded tokenization for testing
Jul 25 09:19:40 :: Unknown tag in saved obj: Ou^T
Jul 25 09:19:40 :: DEBUG: Using hardcoded tokenization for testing  
Jul 25 09:19:40 :: Unknown tag in saved obj: Qu^T
```

The hardcoded test in objsave.c is returning the SAME array for EVERY object:
```c
if (0) {  /* Set to 1 to enable hardcoded test */
  lines = malloc(5 * sizeof(char*));
  lines[0] = strdup("#3183");
  lines[1] = strdup("Loc : -1");
  lines[2] = strdup("Flag: 64 0 0 0");
  lines[3] = strdup("Name: a small leather pouch");
  lines[4] = NULL;
}
```

But the code is set to `if (0)` which means it SHOULDN'T run!

CRITICAL FINDING:
-----------------
The debug message "Using hardcoded tokenization for testing" is being printed, 
but the actual hardcoded array is NOT being used properly. This suggests:

1. The `if (0)` was changed to `if (1)` to enable the test
2. But the parsing code is getting corrupted data anyway
3. The "Unknown tag" messages show garbage: `Ou^T`, `Qu^T`, etc.

The pattern `u^T` suggests memory corruption or uninitialized memory being read.

ROOT CAUSE:
-----------
The hardcoded test is incomplete! It only provides 4 lines for a pouch, but 
the parser expects MANY more lines for a complete object (values, affects, etc).
When the parser reads past line 4, it's reading uninitialized memory!

SOLUTION:
---------
1. DISABLE the hardcoded test: Change `if (1)` back to `if (0)` in objsave.c
2. The original tokenize() is actually working fine for regions/paths
3. The crash is from the incomplete hardcoded test data, not tokenize()!

The garbage tags (Ou^T, Qu^T, etc.) are ASCII values incrementing (O=79, Q=81, etc.)
which strongly suggests reading sequential uninitialized memory.

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!! CRITICAL UPDATE: THE PROBLEM HAS NOT BEEN RESOLVED                     !!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

THE SERVER IS STILL CRASHING WITH THE HARDCODED TEST DISABLED.

The debug output showed:
1. tokenize() works correctly for regions/paths (no empty tokens)
2. The hardcoded test was causing corruption when enabled
3. But with hardcoded test DISABLED, the server STILL CRASHES

THIS MEANS THE ORIGINAL CRASH IS STILL HAPPENING.

The tokenize() function has been improved with error handling, but this did NOT
fix the underlying issue causing the server to crash when loading house data.

NEXT STEPS NEEDED:
1. Need to see ACTUAL tokenize() output when loading house data (not regions)
2. Need to check the raw MySQL data for house #24828
3. May need to add more targeted debugging specifically for house loading
4. The empty first token issue reported by GDB is STILL UNEXPLAINED

THE BUG REMAINS UNRESOLVED.

---

## COMPREHENSIVE DEBUGGING TASK LIST

### Priority 1: Immediate Diagnostics

#### 1. **Isolate House #24828**
- Delete house #24828 from the database temporarily
- `DELETE FROM house_data WHERE vnum = 24828;`
- If server boots successfully, the issue is specific to this house's data

#### 2. **Binary Search Debug**
- Comment out half of the object parsing code in objsave.c
- If it crashes, comment out half of the remaining code
- Continue until the exact crashing line is identified

#### 3. **Validate Database Data**
- Check for hidden Unicode characters:
  `SELECT LENGTH(serialized_obj), CHAR_LENGTH(serialized_obj) FROM house_data WHERE vnum = 24828;`
- If values differ, Unicode characters may be present
- Check data size: `SELECT LENGTH(serialized_obj) FROM house_data WHERE vnum = 24828;`

#### 4. **Memory Corruption Analysis**
- Run under Valgrind: `valgrind --leak-check=full --track-origins=yes bin/circle -q 4101`
- Check for memory corruption occurring before tokenize()

### Priority 2: String Handling Issues

#### 5. **Test Minimal tokenize() Implementation**
```c
// Replace tokenize function temporarily with:
char **tokenize(const char *input, const char *delim) {
    char **result = malloc(2 * sizeof(char*));
    result[0] = strdup("TEST_TOKEN");
    result[1] = NULL;
    return result;
}
```

#### 6. **Thread Safety Check**
- Replace strtok() with strtok_r() for thread-safe tokenization
- strtok() maintains internal state that could be corrupted

#### 7. **Alternative String Duplication**
- Replace strdup() calls with malloc+strcpy
- Some systems have non-standard strdup implementations

### Priority 3: Environmental Factors

#### 8. **Compiler Optimization Issues**
- Recompile with no optimization: `make clean && make CFLAGS="-O0 -g"`
- Modern compilers may optimize assuming no undefined behavior

#### 9. **Stack Size Limitations**
- Increase stack size: `ulimit -s unlimited`
- Check for stack overflow in other parts of the code

#### 10. **Clean Rebuild**
- Delete all object files: `make clean`
- Rebuild entire project: `make`
- Ensures no stale object files

### Priority 4: Database Investigation

#### 11. **Compare Working vs Failing Houses**
- Find a house that loads successfully
- Compare its serialized_obj structure with house #24828
- Identify structural differences

#### 12. **Character Set Analysis**
- Check database character settings: `SHOW VARIABLES LIKE 'character_set%';`
- Ensure consistent encoding between database and application

#### 13. **MySQL Connection Validation**
- Verify connection is active when loading houses
- Add mysql_ping() before queries to ensure connection health

#### 14. **TEXT Field Limitations**
- TEXT fields have 65,535 byte limit
- Check if data is being truncated

### Priority 5: Alternative Approaches

#### 15. **Bypass House Loading Temporarily**
- Comment out house loading in House_load()
- Return early to get server running
- Debug house loading separately

#### 16. **Test Adjacent Houses**
- Try loading house #24827 or #24829
- Determine if issue is specific to one house

#### 17. **Alternative Memory Allocators**
- Test with jemalloc: `LD_PRELOAD=/usr/lib/libjemalloc.so bin/circle -q 4101`
- Different allocators may handle edge cases differently

### Priority 6: Deep Debugging

#### 18. **Macro Redefinition Check**
- Search all headers for redefinitions of free(), malloc(), strdup()
- `grep -r "#define free" *.h`

#### 19. **Raw Data Inspection**
- Extract raw hex data: `SELECT HEX(SUBSTRING(serialized_obj, 1, 100)) FROM house_data WHERE vnum = 24828;`
- Look for unexpected bytes at start of data

#### 20. **Progressive Return Strategy**
- Add early returns throughout the code path
- Start with `return NULL;` at tokenize() entry
- Move return statement down until crash occurs
- Identifies exact location of problem

### Priority 7: External Testing

#### 21. **Different Environment Test**
- Test on different Linux distribution/version
- Environment-specific bugs are possible

#### 22. **MySQL Library Version**
- Check MySQL client library version
- Try static linking with older version if available

### Key Observations
- The bug survives multiple fixes, suggesting the root cause is not where expected
- Empty first token issue remains unexplained
- Hardcoded test corruption was a red herring
- Server still crashes with improved error handling

### Next Immediate Steps
1. Run Priority 1 tasks to isolate the problem
2. If house #24828 specific, examine its data closely
3. If not house specific, focus on memory corruption analysis
4. Document findings at each step for pattern recognition

---

## FIX ATTEMPT #1 (July 25, 2025)
================================

### Changes Made:

1. **Enhanced tokenize() function in mysql.c (lines 221-237)**:
   - Added trimming of leading delimiters to prevent empty first tokens
   - Skip leading delimiters with: `while (*trimmed_input && strchr(delim, *trimmed_input))`
   - Handle edge case where entire string is delimiters
   - Return properly NULL-terminated empty array if needed

2. **Added debugging in objsave.c (lines 2126-2153)**:
   - Log hex dump of first 20 characters for house #24828
   - Log string length
   - Display each character as hex and printable (or '.')
   - Detect and warn about empty first tokens
   - Report house vnum when empty token detected

3. **Added input validation in objsave.c**:
   - Skip empty lines during parsing (line 2161-2163)
   - Added validation before tag parsing (lines 2220-2223, 3150-3153, 3768-3771)
   - Log warning and continue on empty lines
   - Prevents crashes from empty tokens causing garbage in tag parsing

### Expected Behavior:
- Server should not crash on empty first tokens
- Debug output will show if house #24828 data starts with newline (0x0A)
- Empty lines will be gracefully skipped
- Better error messages for troubleshooting

### Testing:
- Code compiles successfully
- Run server and check logs for house #24828 debug output
- Look for patterns like:
  ```
  DEBUG: House #24828 raw data first 20 chars (hex):
  DEBUG:   [0] = 0x0A ('.')
  WARNING: Empty first token detected in house data
  ```

### Status: ATTEMPT - NOT YET VERIFIED
This fix addresses the most likely cause (leading newlines in MySQL data) but needs testing to confirm.

---

## CORE DUMP ANALYSIS (July 25, 2025 - House #24828)
===================================================

### Debug Output Results:
The debug code successfully captured the raw data from house #24828:
```
DEBUG: House #24828 raw data first 20 chars (hex):
DEBUG: String length: 284
DEBUG:   [0] = 0x23 ('#')
DEBUG:   [1] = 0x33 ('3')
DEBUG:   [2] = 0x31 ('1')
DEBUG:   [3] = 0x38 ('8')
DEBUG:   [4] = 0x33 ('3')
DEBUG:   [5] = 0x0A ('.')    <- newline (correct)
DEBUG:   [6] = 0x4C ('L')
DEBUG:   [7] = 0x6F ('o')
DEBUG:   [8] = 0x63 ('c')
...
```

**FINDING**: The data is CORRECT! It starts with "#3183" - no leading newline issue.

### Core Dump Stack Trace:
```
#0  0x00007f7afd66c387 in raise () from /lib64/libc.so.6
#1  0x00007f7afd66da78 in abort () from /lib64/libc.so.6
#2  0x00007f7afd6aef67 in __libc_message () from /lib64/libc.so.6
#3  0x00007f7afd6b7329 in _int_free () from /lib64/libc.so.6
#4  0x00000000004d2669 in free_tokens (tokens=0x244b9a70) at mysql.c:343
#5  0x00000000005d2fc5 in objsave_parse_objects_db (name=0x0, house_vnum=24828) at objsave.c:2454
#6  0x00000000005df9ad in House_load (vnum=24828) at house.c:174
```

### THE REAL BUG - MEMORY CORRUPTION IN PARSING CODE
====================================================

#### Discovery Process:

1. **Initial Investigation**:
   - Crash in free_tokens() trying to free corrupted memory
   - tokens[0] = "0\233K$" (garbage instead of expected string)
   - All token strings corrupted with similar patterns

2. **Memory Layout Analysis**:
   ```
   tokens array at 0x244b9a70:
   0x244b9a70:     0x244b98a0      0x00000000      0x244b98c0      0x00000000
   0x244b9a80:     0x244b98e0      0x00000000      0x244b9900      0x00000000
   ```
   - Pointers are correctly stored as 64-bit values
   - No 32/64-bit compatibility issue

3. **Corruption Pattern Discovered**:
   Each token string has its first 8 bytes overwritten with a pointer value:
   ```
   0x244b98a0:     0x30 0x9b 0x4b 0x24 0x00 0x00 0x00 0x00  <- corrupted
   0x244b98c0:     0x90 0x98 0x4b 0x24 0x00 0x00 0x00 0x00  <- corrupted
   0x244b98e0:     0xb0 0x98 0x4b 0x24 0x00 0x00 0x00 0x00  <- corrupted
   ```

4. **Smoking Gun**:
   At address 0x244b9990 (tokens[5]):
   ```
   0x244b9990:     0x30 0x98 0x4b 0x24 0x00 0x00 0x00 0x00  <- corruption
   0x244b9998:     0x73 0x69 0x6c 0x6b 0x20 0x63 0x61 0x70  <- "silk cap"
   ```
   The original string content "silk cape" is still there, but the first 8 bytes
   have been overwritten with what looks like a pointer (0x244b9830).

#### Root Cause Analysis:

**The bug is NOT in tokenize() or free_tokens()!**

The tokenize() function is working correctly. The corruption happens AFTER 
tokenization, during the object parsing loop (lines 2158-2454 in objsave.c).

Something in the parsing code is:
1. Treating the token strings as if they were pointers to pointers
2. Writing 8-byte values (pointers) through them
3. Overwriting the beginning of each string with pointer values

This is likely caused by incorrect use of sscanf() or similar functions where
the destination is incorrectly specified, such as:
```c
sscanf(*line, "format", line);   // WRONG - writes through the pointer
```
Instead of:
```c
sscanf(*line, "format", buffer); // CORRECT - writes to a buffer
```

#### Evidence Supporting This Theory:

1. The serialized_obj data is intact and correct
2. The tokens array structure is valid
3. The corruption is systematic - exactly 8 bytes at the start of each string
4. The corruption contains what appear to be valid pointer values
5. The rest of the string data after the first 8 bytes is intact

### CONCLUSION
=============

The crash is caused by memory corruption in the object parsing code, NOT by
the tokenize function. The parsing code is incorrectly writing pointer values
over the beginning of the token strings, corrupting them before free_tokens()
is called.

### NEXT STEPS FOR FIXING
========================

1. Search objsave.c parsing loop (lines 2158-2454) for incorrect pointer usage
2. Look for sscanf() or similar functions using token pointers as destinations
3. Check tag_argument() function and how it handles the string pointers
4. Verify all parsing operations write to appropriate buffers, not through token pointers

The fix will involve correcting the parsing code to not overwrite the token strings.

---

## ACTUAL FIX IMPLEMENTED (July 25, 2025)
==========================================

### THE REAL BUG: Double-Free in objsave.c

After extensive investigation, the actual bug was found:

**Bug Location**: objsave.c lines 2416, 3352, 3976 (and other locations)
```c
free(*line);  // WRONG! This frees individual token strings
```

**The Problem**:
1. The parsing loop was calling `free(*line)` to free individual token strings
2. These strings were allocated by tokenize() and belong to the lines array
3. At the end of parsing (line 2454), `free_tokens(lines)` is called
4. free_tokens() tries to free all the strings again, causing a double-free

**Memory Corruption Pattern Explained**:
1. During parsing, `free(*line)` frees the token strings
2. The memory allocator reuses this freed memory for other allocations
3. New allocations write pointer values into the old string memory
4. When free_tokens() runs, it sees corrupted data (the pointer values)
5. Attempting to free these corrupted "strings" causes the crash

### Fix Applied:
Removed all `free(*line)` calls in the parsing loops and replaced them with comments:
```c
/* DO NOT free(*line) here - the lines array will be freed by free_tokens() */
```

### Files Modified:
- objsave.c: Removed free(*line) calls at lines 2416, 3352, 3976 and all similar locations

### Key Lesson:
When using tokenize(), the returned array owns all the token strings. Individual strings should NOT be freed during processing - only free_tokens() should be called at the end to properly clean up the entire array.

The tokenize() function itself was working correctly all along. The bug was a classic double-free error in the code that used tokenize().